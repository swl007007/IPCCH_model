{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_9964\\1679473219.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['month'] = pd.DatetimeIndex(test_df['date']).month\n",
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_9964\\1679473219.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['year'] = pd.DatetimeIndex(test_df['date']).year\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "# import random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#import linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import tqdm\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "#import r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "#import confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import roc auc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from food_crisis_functions import *\n",
    "import json\n",
    "with open(\"forecasting_hyperparameters.json\", \"r\") as file:\n",
    "    best_params_xgb_regressor= json.load(file)\n",
    "    \n",
    "with open(\"forecasting_hyperparameters_p3.json\", \"r\") as file:\n",
    "    best_params_xgb_regressor_for_p3= json.load(file)\n",
    "# read csv\n",
    "df = pd.read_csv(r'C:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\1.Source Data\\forecasting_subset_IPCCH_v1210.csv')\n",
    "# add dummys for area_id and month year\n",
    "#df = pd.concat([df, pd.get_dummies(df['area_id'], prefix='area_id')], axis=1)\n",
    "#df = pd.concat([df, pd.get_dummies(df['date'], prefix='month_year')], axis=1)\n",
    "# drop lat and lon\n",
    "#df = df.drop(['lat', 'lon'], axis=1)\n",
    "###drop fews_ipc_ha\n",
    "#df = df.drop(['fews_ipc_ha'], axis=1)\n",
    "# random split train and test\n",
    "\n",
    "# prepare date from year and month\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(DAY=1))\n",
    "# check for inf and replace with na\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# replace df['overall_phase], 0 as 1, >5 as 5\n",
    "df['overall_phase'] = df['overall_phase'].apply(lambda x: 1 if x < 1 else (5 if x > 5 else x))\n",
    "\n",
    "df_origin = df.copy()\n",
    "y_pred_test = pd.DataFrame()\n",
    "model_stats = pd.DataFrame()\n",
    "#select phase1_percent is not na\n",
    "df = df_origin[df_origin['phase1_percent'].notna()]\n",
    "\n",
    "# Sort by region and date\n",
    "df = df.sort_values(by=['area_id', 'date'])\n",
    "#drop overall phase\n",
    "df = df.drop(['overall_phase'], axis=1)\n",
    "#for each region, set last observation to be test set\n",
    "# create a series of new outcome, phase2_worse=phase2_percent+phase3_percent+phase4_percent+phase5_percent, phase3_worse=phase3_percent+phase4_percent+phase5_percent, phase4_worse=phase4_percent+phase5_percent, phase5_worse=phase5_percent\n",
    "df['phase2_worse'] = df['phase2_percent'] + df['phase3_percent'] + df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase3_worse'] = df['phase3_percent'] + df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase4_worse'] = df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase5_worse'] = df['phase5_percent']\n",
    "#drop phase2_percent, phase3_percent, phase4_percent, phase5_percent, phase1_percent\n",
    "df = df.drop(['phase2_percent', 'phase3_percent', 'phase4_percent', 'phase5_percent', 'phase1_percent'], axis=1)\n",
    "# Splitting the data\n",
    "#test_df = df.groupby('area_id').tail(1)\n",
    "#train_df = df.drop(test_df.index)\n",
    "#test_df = test_df.drop(['area_id','date'], axis=1)\n",
    "#train_df = train_df.drop(['area_id','date'], axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data\n",
    "#test_df = df.groupby('area_id').tail(1)\n",
    "#train_df = df.drop(test_df.index)\n",
    "#test_df = test_df.drop(['area_id','date'], axis=1)\n",
    "#train_df = train_df.drop(['area_id','date'], axis=1)\n",
    "y_pred_test = pd.DataFrame()\n",
    "# drop anything after 2022-01-01\n",
    "#df = df[df['date'] < '2021-01-01']\n",
    "shape_values_df_ensemble = pd.DataFrame()\n",
    "df_result = pd.DataFrame()\n",
    "date = \"2022-01-01\"  # Define the 'date' variable\n",
    "cutoff_date = \"2023-01-01\"\n",
    "#order unique_dates\n",
    "y_pred_test=pd.DataFrame()\n",
    "\n",
    "for i in range(2, 6):\n",
    "    train_df = df[df['date'] < date]\n",
    "    test_df = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "    train_df = train_df.drop(['date','area_id'], axis=1)\n",
    "    test_df = test_df.drop(['date','area_id'], axis=1)\n",
    "    train_df_new = train_df.drop(['phase{}_worse'.format(j) for j in range(2, 6) if j != i], axis=1)\n",
    "    test_df_new = test_df.drop(['phase{}_worse'.format(j) for j in range(2, 6) if j != i], axis=1)\n",
    "# drop rows with NaN in phase{}_percent\n",
    "    train_df_new = train_df_new.dropna(subset=['phase{}_worse'.format(i)])\n",
    "    test_df_new = test_df_new.dropna(subset=['phase{}_worse'.format(i)])\n",
    "    test_index = test_df_new.index\n",
    "    # Split into features and target\n",
    "    X_train = train_df_new.drop('phase{}_worse'.format(i), axis=1)\n",
    "    y_train = train_df_new['phase{}_worse'.format(i)]\n",
    "    X_test = test_df_new.drop('phase{}_worse'.format(i), axis=1)\n",
    "    y_test = test_df_new['phase{}_worse'.format(i)]\n",
    "    #fews_ipc_ha_test = X_test['fews_ipc_ha']\n",
    "    #X_train = X_train.drop(['fews_ipc_ha'], axis=1)\n",
    "    #X_test = X_test.drop(['fews_ipc_ha'], axis=1)\n",
    "    if i == 3:\n",
    "        best_params_xgb_regressor = best_params_xgb_regressor_for_p3\n",
    "    model = xgb.XGBRegressor(**best_params_xgb_regressor)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # for y_pred_test, add a column to indicate the phase\n",
    "    #y_pred_test = pd.concat([y_pred_test, pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'phase': [i]*len(y_pred),'fews_ipc_ha':fews_ipc_ha_test,'test_index':test_index})], ignore_index=True)\n",
    "    y_pred_test = pd.concat([y_pred_test, pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'phase': [i]*len(y_pred),'test_index':test_index})], ignore_index=True)\n",
    "   \n",
    "    # shap values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    #save shap values\n",
    "    shap_values_df = pd.DataFrame(shap_values, columns=X_train.columns)\n",
    "    \n",
    "    # add a column to indicate the phase\n",
    "    shap_values_df['phase'] = i\n",
    "    \n",
    "    # append to shape_values_df_ensemble\n",
    "    shape_values_df_ensemble = pd.concat([shape_values_df_ensemble, shap_values_df], ignore_index=True)\n",
    "y_pred_test = convert_prob_to_phase(y_pred_test)\n",
    "y_test = y_pred_test['overall_phase']\n",
    "y_pred = y_pred_test['overall_phase_pred']\n",
    "\n",
    "test_df = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "#generate month, year in test_df\n",
    "test_df['month'] = pd.DatetimeIndex(test_df['date']).month\n",
    "test_df['year'] = pd.DatetimeIndex(test_df['date']).year\n",
    "# keep only the last three columns\n",
    "y_pred_test_new = y_pred_test.iloc[:,-3:]\n",
    "#select only area_id, month, year in test_df\n",
    "test_df = test_df[['area_id', 'month', 'year']]\n",
    "#test_df reset index\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "# concate horizontally test_df and y_pred_test_new, ignore index\n",
    "df_result = pd.concat([test_df, y_pred_test_new], axis=1, ignore_index=True)\n",
    "# rename columns\n",
    "df_result.columns = ['area_id', 'month', 'year', 'test_index','overall_phase','overall_phase_pred']\n",
    "# use month and year to create date\n",
    "df_result['date'] = pd.to_datetime(df_result[['year', 'month']].assign(day=1))\n",
    "#drop month and year\n",
    "df_result = df_result.drop(['month', 'year'], axis=1)\n",
    "# select date, area_id, overall_phase in df\n",
    "df = pd.read_csv(r'C:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\1.Source Data\\forecasting_subset_IPCCH_v1210.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_9964\\1861004737.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('area_id').apply(lambda x: x.nlargest(2, 'count')).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(DAY=1))\n",
    "df = df[['date', 'area_id', 'overall_phase']]\n",
    "# convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df_result drop test_index\n",
    "df_result = df_result.drop(['test_index'], axis=1)\n",
    "# df add a column overall_phase_pred, set it equal to overall_phase\n",
    "df['overall_phase_pred'] = df['overall_phase']\n",
    "# df drop any row after 2024-01-01\n",
    "df = df[df['date'] < '2022-01-01']\n",
    "# concate df and df_result\n",
    "df = pd.concat([df, df_result], ignore_index=True)\n",
    "# add a new column count the nth time the area_id appears\n",
    "df['count'] = df.groupby('area_id').cumcount() + 1\n",
    "# groupby area_id, keep rows with two largest count\n",
    "df = df.groupby('area_id').apply(lambda x: x.nlargest(2, 'count')).reset_index(drop=True)\n",
    "# set all count==1 to 0, set all count>1 ro 1\n",
    "df['count'] = df['count'].apply(lambda x: 0 if x == 1 else 1)\n",
    "#group by area_id, if overall_phase of count==1 is the same as overall_phase of count==0, drop all rows associated with area_id,but if there is only one row associated with area_id, keep it\n",
    "df_1 = df.groupby('area_id').filter(lambda x: x['overall_phase'].nunique() > 1)\n",
    "df_2 = df.groupby('area_id').filter(lambda x: x['area_id'].count() == 1)\n",
    "# concate df_1 and df_2, sort by date and area_id\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "df = df.sort_values(by=['date', 'area_id'])\n",
    "# for each area_id, keep only the last row\n",
    "df = df.groupby('area_id').tail(1)\n",
    "#drop count\n",
    "df = df.drop(['count'], axis=1)\n",
    "# drop any row before 2024-01-01\n",
    "df_after = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "#calculate accuracy score using overall_phase and overall_phase_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.4955223880597015, 0.5758835758835759, 0.647196261682243)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#drop missing in df_after\n",
    "df_after = df_after.dropna(subset=['overall_phase', 'overall_phase_pred'])\n",
    "accuracy = accuracy_score(df_after['overall_phase'], df_after['overall_phase_pred'])\n",
    "# set phase 1 and phase 2 as negative, phase 3, 4, 5 as positive, calculate sensitivity(recall) and presicion\n",
    "cm = confusion_matrix(df_after['overall_phase'], df_after['overall_phase_pred'])\n",
    "correct_3_more = np.sum(cm[2:, 2:])\n",
    "total_3_more = np.sum(cm[2:, :])\n",
    "sensitivity = correct_3_more / total_3_more if total_3_more != 0 else 0\n",
    "\n",
    "total_3_more_pred = np.sum(cm[:, 2:])\n",
    "precision = correct_3_more / total_3_more_pred if total_3_more_pred != 0 else 0\n",
    "accuracy,sensitivity, precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

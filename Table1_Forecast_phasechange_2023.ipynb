{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_41508\\1435165756.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['month'] = pd.DatetimeIndex(test_df['date']).month\n",
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_41508\\1435165756.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['year'] = pd.DatetimeIndex(test_df['date']).year\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "# import random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#import linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import tqdm\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "#import r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "#import confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import roc auc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from food_crisis_functions import *\n",
    "import json\n",
    "with open(\"forecasting_hyperparameters.json\", \"r\") as file:\n",
    "    best_params_xgb_regressor= json.load(file)\n",
    "    \n",
    "with open(\"forecasting_hyperparameters_p3.json\", \"r\") as file:\n",
    "    best_params_xgb_regressor_for_p3= json.load(file)\n",
    "# read csv\n",
    "df = pd.read_csv(r'C:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\1.Source Data\\forecasting_subset_IPCCH_v1210.csv')\n",
    "# add dummys for area_id and month year\n",
    "#df = pd.concat([df, pd.get_dummies(df['area_id'], prefix='area_id')], axis=1)\n",
    "#df = pd.concat([df, pd.get_dummies(df['date'], prefix='month_year')], axis=1)\n",
    "# drop lat and lon\n",
    "#df = df.drop(['lat', 'lon'], axis=1)\n",
    "###drop fews_ipc_ha\n",
    "#df = df.drop(['fews_ipc_ha'], axis=1)\n",
    "# random split train and test\n",
    "\n",
    "# prepare date from year and month\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(DAY=1))\n",
    "# check for inf and replace with na\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# replace df['overall_phase], 0 as 1, >5 as 5\n",
    "df['overall_phase'] = df['overall_phase'].apply(lambda x: 1 if x < 1 else (5 if x > 5 else x))\n",
    "\n",
    "df_origin = df.copy()\n",
    "y_pred_test = pd.DataFrame()\n",
    "model_stats = pd.DataFrame()\n",
    "#select phase1_percent is not na\n",
    "df = df_origin[df_origin['phase1_percent'].notna()]\n",
    "\n",
    "# Sort by region and date\n",
    "df = df.sort_values(by=['area_id', 'date'])\n",
    "#drop overall phase\n",
    "df = df.drop(['overall_phase'], axis=1)\n",
    "#for each region, set last observation to be test set\n",
    "# create a series of new outcome, phase2_worse=phase2_percent+phase3_percent+phase4_percent+phase5_percent, phase3_worse=phase3_percent+phase4_percent+phase5_percent, phase4_worse=phase4_percent+phase5_percent, phase5_worse=phase5_percent\n",
    "df['phase2_worse'] = df['phase2_percent'] + df['phase3_percent'] + df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase3_worse'] = df['phase3_percent'] + df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase4_worse'] = df['phase4_percent'] + df['phase5_percent']\n",
    "df['phase5_worse'] = df['phase5_percent']\n",
    "#drop phase2_percent, phase3_percent, phase4_percent, phase5_percent, phase1_percent\n",
    "df = df.drop(['phase2_percent', 'phase3_percent', 'phase4_percent', 'phase5_percent', 'phase1_percent'], axis=1)\n",
    "# Splitting the data\n",
    "#test_df = df.groupby('area_id').tail(1)\n",
    "#train_df = df.drop(test_df.index)\n",
    "#test_df = test_df.drop(['area_id','date'], axis=1)\n",
    "#train_df = train_df.drop(['area_id','date'], axis=1)\n",
    "\n",
    "\n",
    "# Splitting the data\n",
    "#test_df = df.groupby('area_id').tail(1)\n",
    "#train_df = df.drop(test_df.index)\n",
    "#test_df = test_df.drop(['area_id','date'], axis=1)\n",
    "#train_df = train_df.drop(['area_id','date'], axis=1)\n",
    "y_pred_test = pd.DataFrame()\n",
    "# drop anything after 2022-01-01\n",
    "#df = df[df['date'] < '2021-01-01']\n",
    "shape_values_df_ensemble = pd.DataFrame()\n",
    "df_result = pd.DataFrame()\n",
    "date = \"2023-01-01\"  # Define the 'date' variable\n",
    "cutoff_date = \"2024-01-01\"\n",
    "#order unique_dates\n",
    "y_pred_test=pd.DataFrame()\n",
    "\n",
    "for i in range(2, 6):\n",
    "    train_df = df[df['date'] < date]\n",
    "    test_df = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "    train_df = train_df.drop(['date','area_id'], axis=1)\n",
    "    test_df = test_df.drop(['date','area_id'], axis=1)\n",
    "    train_df_new = train_df.drop(['phase{}_worse'.format(j) for j in range(2, 6) if j != i], axis=1)\n",
    "    test_df_new = test_df.drop(['phase{}_worse'.format(j) for j in range(2, 6) if j != i], axis=1)\n",
    "# drop rows with NaN in phase{}_percent\n",
    "    train_df_new = train_df_new.dropna(subset=['phase{}_worse'.format(i)])\n",
    "    test_df_new = test_df_new.dropna(subset=['phase{}_worse'.format(i)])\n",
    "    test_index = test_df_new.index\n",
    "    # Split into features and target\n",
    "    X_train = train_df_new.drop('phase{}_worse'.format(i), axis=1)\n",
    "    y_train = train_df_new['phase{}_worse'.format(i)]\n",
    "    X_test = test_df_new.drop('phase{}_worse'.format(i), axis=1)\n",
    "    y_test = test_df_new['phase{}_worse'.format(i)]\n",
    "    #fews_ipc_ha_test = X_test['fews_ipc_ha']\n",
    "    #X_train = X_train.drop(['fews_ipc_ha'], axis=1)\n",
    "    #X_test = X_test.drop(['fews_ipc_ha'], axis=1)\n",
    "    if i == 3:\n",
    "        best_params_xgb_regressor = best_params_xgb_regressor_for_p3\n",
    "    model = xgb.XGBRegressor(**best_params_xgb_regressor)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # for y_pred_test, add a column to indicate the phase\n",
    "    #y_pred_test = pd.concat([y_pred_test, pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'phase': [i]*len(y_pred),'fews_ipc_ha':fews_ipc_ha_test,'test_index':test_index})], ignore_index=True)\n",
    "    y_pred_test = pd.concat([y_pred_test, pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'phase': [i]*len(y_pred),'test_index':test_index})], ignore_index=True)\n",
    "   \n",
    "    # shap values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    #save shap values\n",
    "    shap_values_df = pd.DataFrame(shap_values, columns=X_train.columns)\n",
    "    \n",
    "    # add a column to indicate the phase\n",
    "    shap_values_df['phase'] = i\n",
    "    \n",
    "    # append to shape_values_df_ensemble\n",
    "    shape_values_df_ensemble = pd.concat([shape_values_df_ensemble, shap_values_df], ignore_index=True)\n",
    "y_pred_test = convert_prob_to_phase(y_pred_test)\n",
    "y_test = y_pred_test['overall_phase']\n",
    "y_pred = y_pred_test['overall_phase_pred']\n",
    "\n",
    "test_df = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "#generate month, year in test_df\n",
    "test_df['month'] = pd.DatetimeIndex(test_df['date']).month\n",
    "test_df['year'] = pd.DatetimeIndex(test_df['date']).year\n",
    "# keep only the last three columns\n",
    "y_pred_test_new = y_pred_test.iloc[:,-3:]\n",
    "#select only area_id, month, year in test_df\n",
    "test_df = test_df[['area_id', 'month', 'year']]\n",
    "#test_df reset index\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "# concate horizontally test_df and y_pred_test_new, ignore index\n",
    "df_result = pd.concat([test_df, y_pred_test_new], axis=1, ignore_index=True)\n",
    "# rename columns\n",
    "df_result.columns = ['area_id', 'month', 'year', 'test_index','overall_phase','overall_phase_pred']\n",
    "# use month and year to create date\n",
    "df_result['date'] = pd.to_datetime(df_result[['year', 'month']].assign(day=1))\n",
    "#drop month and year\n",
    "df_result = df_result.drop(['month', 'year'], axis=1)\n",
    "# select date, area_id, overall_phase in df\n",
    "df = pd.read_csv(r'C:\\Users\\swl00\\IFPRI Dropbox\\Weilun Shi\\Google fund\\Analysis\\1.Source Data\\forecasting_subset_IPCCH_v1210.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swl00\\AppData\\Local\\Temp\\ipykernel_41508\\3088461260.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('area_id').apply(lambda x: x.nlargest(2, 'count')).reset_index(drop=True)\n",
      "C:\\Users\\swl00\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:399: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m df_after = df[(df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] < cutoff_date) & (df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] >= date)]\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#calculate accuracy score using overall_phase and overall_phase_pred\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m accuracy = \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_after\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverall_phase\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_after\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverall_phase_pred\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# set phase 1 and phase 2 as negative, phase 3, 4, 5 as positive, calculate sensitivity(recall) and presicion\u001b[39;00m\n\u001b[32m     34\u001b[39m cm = confusion_matrix(df_after[\u001b[33m'\u001b[39m\u001b[33moverall_phase\u001b[39m\u001b[33m'\u001b[39m], df_after[\u001b[33m'\u001b[39m\u001b[33moverall_phase_pred\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:227\u001b[39m, in \u001b[36maccuracy_score\u001b[39m\u001b[34m(y_true, y_pred, normalize, sample_weight)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[32m    226\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type.startswith(\u001b[33m\"\u001b[39m\u001b[33mmultilabel\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:99\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     97\u001b[39m xp, _ = get_namespace(y_true, y_pred)\n\u001b[32m     98\u001b[39m check_consistent_length(y_true, y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m type_true = \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_true\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m y_type = {type_true, type_pred}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\multiclass.py:417\u001b[39m, in \u001b[36mtype_of_target\u001b[39m\u001b[34m(y, input_name, raise_unknown)\u001b[39m\n\u001b[32m    415\u001b[39m     data = y.data \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xp.any(data != xp.astype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontinuous\u001b[39m\u001b[33m\"\u001b[39m + suffix\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(DAY=1))\n",
    "df = df[['date', 'area_id', 'overall_phase']]\n",
    "# convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# df_result drop test_index\n",
    "df_result = df_result.drop(['test_index'], axis=1)\n",
    "# df add a column overall_phase_pred, set it equal to overall_phase\n",
    "df['overall_phase_pred'] = df['overall_phase']\n",
    "# df drop any row after 2024-01-01\n",
    "df = df[df['date'] < '2023-01-01']\n",
    "# concate df and df_result\n",
    "df = pd.concat([df, df_result], ignore_index=True)\n",
    "# add a new column count the nth time the area_id appears\n",
    "df['count'] = df.groupby('area_id').cumcount() + 1\n",
    "# groupby area_id, keep rows with two largest count\n",
    "df = df.groupby('area_id').apply(lambda x: x.nlargest(2, 'count')).reset_index(drop=True)\n",
    "# set all count==1 to 0, set all count>1 ro 1\n",
    "df['count'] = df['count'].apply(lambda x: 0 if x == 1 else 1)\n",
    "#group by area_id, if overall_phase of count==1 is the same as overall_phase of count==0, drop all rows associated with area_id,but if there is only one row associated with area_id, keep it\n",
    "df_1 = df.groupby('area_id').filter(lambda x: x['overall_phase'].nunique() > 1)\n",
    "df_2 = df.groupby('area_id').filter(lambda x: x['area_id'].count() == 1)\n",
    "# concate df_1 and df_2, sort by date and area_id\n",
    "df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "df = df.sort_values(by=['date', 'area_id'])\n",
    "# for each area_id, keep only the last row\n",
    "df = df.groupby('area_id').tail(1)\n",
    "#drop count\n",
    "df = df.drop(['count'], axis=1)\n",
    "# drop any row before 2024-01-01\n",
    "df_after = df[(df['date'] < cutoff_date) & (df['date'] >= date)]\n",
    "#calculate accuracy score using overall_phase and overall_phase_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.5101754385964913, 0.788218793828892, 0.7033792240300375)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#drop missing in df_after\n",
    "df_after = df_after.dropna(subset=['overall_phase', 'overall_phase_pred'])\n",
    "accuracy = accuracy_score(df_after['overall_phase'], df_after['overall_phase_pred'])\n",
    "# set phase 1 and phase 2 as negative, phase 3, 4, 5 as positive, calculate sensitivity(recall) and presicion\n",
    "cm = confusion_matrix(df_after['overall_phase'], df_after['overall_phase_pred'])\n",
    "correct_3_more = np.sum(cm[2:, 2:])\n",
    "total_3_more = np.sum(cm[2:, :])\n",
    "sensitivity = correct_3_more / total_3_more if total_3_more != 0 else 0\n",
    "\n",
    "total_3_more_pred = np.sum(cm[:, 2:])\n",
    "precision = correct_3_more / total_3_more_pred if total_3_more_pred != 0 else 0\n",
    "accuracy,sensitivity, precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
